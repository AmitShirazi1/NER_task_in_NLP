{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "\n",
    "vec_num = 100\n",
    "GLOVE_PATH = f'glove-twitter-{vec_num}'\n",
    "glove_twitter = downloader.load(GLOVE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def open_and_split_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        words_str = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in lines:\n",
    "            try:\n",
    "                word, tag = line.rstrip().split(\"\\t\")\n",
    "                word = word.lower()\n",
    "                if (word not in glove_twitter):\n",
    "                    words.append(np.zeros(vec_num))\n",
    "                else:\n",
    "                    words.append(glove_twitter[word])\n",
    "                tags.append(0 if tag == \"O\" else 1)\n",
    "                words_str.append(word)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_labels = open_and_split_file(\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words, dev_labels = open_and_split_file(\"/home/student/hw2/NER_task_in_NLP/data/dev.tagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5422804146208401"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(train_words, train_labels)\n",
    "y_pred = knn.predict(dev_words)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(dev_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        # Create a path-to-label dictionary\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "            words = []\n",
    "            tags = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    word, tag = line.rstrip().split(\"\\t\")\n",
    "                    word = word.lower()\n",
    "                    if (word not in glove_twitter):\n",
    "                        words.append(np.zeros(vec_num))\n",
    "                    else:\n",
    "                        words.append(glove_twitter[word])\n",
    "                    tags.append(0 if tag == \"O\" else 1)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word = self.words[index]\n",
    "        tag = self.tags[index]\n",
    "        word = torch.FloatTensor(word).squeeze()\n",
    "        data = {\"word\": word, \"labels\": tag}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(path=\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\")\n",
    "dev_dataset = CustomDataset(path=\"/home/student/hw2/NER_task_in_NLP/data/dev.tagged\")\n",
    "datasets = {\"train\": train_dataset, \"dev\": dev_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, vec_dim, num_classes, hidden_dim=100):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.first_layer = nn.Linear(vec_dim, hidden_dim)\n",
    "        self.second_layer = nn.Linear(hidden_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, word, labels=None):\n",
    "        x = self.first_layer(word)\n",
    "        x = self.activation(x)\n",
    "        x = self.second_layer(x)\n",
    "        if labels is None:\n",
    "            return x, None\n",
    "        loss = self.loss(x, labels)\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, data_sets, optimizer, num_epochs: int, batch_size=16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_loaders = {\"train\": DataLoader(data_sets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "                    \"dev\": DataLoader(data_sets[\"dev\"], batch_size=batch_size, shuffle=False)}\n",
    "    model.to(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch in data_loaders['train']:\n",
    "            batch_size = 0\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "                batch_size = v.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model(**batch)\n",
    "            # loss.backward()  # The important part\n",
    "            optimizer.step()\n",
    "                \n",
    "    # Now use the dev dataset to evaluate the model.\n",
    "    model.eval()\n",
    "    predictions = torch.tensor([])\n",
    "    tags = torch.tensor([])\n",
    "    for batch in data_loaders['dev']:\n",
    "        batch_size = 0\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "            batch_size = v.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()    \n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(**batch)\n",
    "            pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "            predictions = torch.cat((predictions, pred), 0)\n",
    "        tags = torch.cat((tags, (batch[\"labels\"].clone().detach().cpu())), 0)\n",
    "    \n",
    "    score = f1_score(tags, predictions)\n",
    "    print(f'F1 score: {score}')\n",
    "                \n",
    "    # with open('model.pkl', 'rb') as f:\n",
    "    #     model = torch.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24772/3324396507.py:29: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  word = torch.FloatTensor(word).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5432989690721649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters that we can change:\n",
    "# hidden_dim: the dimension of the hidden layer\n",
    "# num_epochs: the number of epochs to train the model\n",
    "# learning_rate: the learning rate of the optimizer (Adam) - find out more about it in the documentation.\n",
    "model = FeedForwardNN(vec_num, 2, hidden_dim=int(vec_num*2))\n",
    "optimizer = Adam(params=model.parameters())\n",
    "model = train(model=model, data_sets=datasets, optimizer=optimizer, num_epochs=15)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "X_train_tensors = Variable(torch.Tensor(np.array(train_words)))\n",
    "X_dev_tensors = Variable(torch.Tensor(np.array(dev_words)))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(np.array(train_labels)))\n",
    "y_dev_tensors = Variable(torch.Tensor(np.array(dev_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_dev_tensors = torch.reshape(X_dev_tensors, (X_dev_tensors.shape[0], 1, X_dev_tensors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This code was inspired by the following source: https://cnvrg.io/pytorch-lstm/\n",
    "    Because we thought it was more suitable for our uses than the one studied in class. '''\n",
    "        \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, hidden2_size, num_stacked_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes  # We have 2 classes, binary.\n",
    "        self.input_size = input_size  # The number of expected features in the input x.\n",
    "        self.hidden_size = hidden_size  # number of features in hidden state.\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        self.seq_length = seq_length  # Number of words in each timestamp.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)  # lstm\n",
    "        self.layer1 =  nn.Linear(hidden_size, hidden2_size)  # Layer 1 in the LSTM\n",
    "        self.layer2 = nn.Linear(hidden2_size, num_classes)  # Layer 2 in the LSTM\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, word, labels=None):\n",
    "        word = word.unsqueeze(1)\n",
    "        h_0 = Variable(torch.zeros(self.num_stacked_layers, word.size(0), self.hidden_size)).to(self.device)  # Short term memory.\n",
    "        c_0 = Variable(torch.zeros(self.num_stacked_layers, word.size(0), self.hidden_size)).to(self.device)  # Long term memory.\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(word, (h_0, c_0))  # Perform lstm with relation to input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size)  # Reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.layer1(out)  # First Dense\n",
    "        out = self.relu(out)  # Activation function - Relu\n",
    "        out = self.layer2(out)  # Second layer\n",
    "        out = self.softmax(out) # Activation function - Softmax\n",
    "        # pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "        if labels is None:\n",
    "            return out, None\n",
    "        loss = self.loss(out, labels)\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_stacked_layers = 1  # Number of stacked lstm layers, in this model, we do not stack layers.\n",
    "num_classes = 2  # Number of output classes\n",
    "\n",
    "lstm = LSTM(num_classes, vec_num, int(vec_num/2), vec_num*2, num_stacked_layers, X_train_tensors.shape[1])  # Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()  # Cross entropy loss\n",
    "optimizer = Adam(lstm.parameters(), lr=learning_rate)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24772/3703993139.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.softmax(out) # Activation function - Softmax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5894627021387585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm = train(model=lstm, data_sets=datasets, optimizer=optimizer, num_epochs=15)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def open_split_file_and_calc_weights(file_path):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        words_str = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        word2idx = {\"null\" : 0}\n",
    "        weights = [np.zeros(vec_num)]\n",
    "        for line in lines:\n",
    "            try:\n",
    "                vector_added = False\n",
    "                word, tag = line.rstrip().split(\"\\t\")\n",
    "                word = word.lower()\n",
    "                words_str.append(word)\n",
    "                tags.append(0 if tag == \"O\" else 1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for w_idx, word in enumerate(words_str):\n",
    "            if word not in word2idx.keys():\n",
    "                vector_added = True\n",
    "                word2idx[word] = len(word2idx.keys())\n",
    "\n",
    "            if (word not in glove_twitter):\n",
    "                stemmed_words = list()\n",
    "                # try stemming\n",
    "                for i in range(min(len(word), 5)):\n",
    "                    if word[i:] in glove_twitter:\n",
    "                        stemmed_words.append(glove_twitter[word[i:]])\n",
    "                    for j in range(1, min(len(word), 5)):\n",
    "                        if word[i:-j] in glove_twitter:\n",
    "                            stemmed_words.append(glove_twitter[word[i:-j]])\n",
    "                average_on_stemmed = np.mean(np.array(stemmed_words), axis=0) if stemmed_words else np.zeros(vec_num)\n",
    "\n",
    "                window_words = list()\n",
    "                for i in range(1, 3):\n",
    "                    if len(words) > i:\n",
    "                        window_words.append(words[-i])\n",
    "                    if w_idx < len(words_str) - i:\n",
    "                        if words_str[w_idx + i] in glove_twitter:\n",
    "                            window_words.append(glove_twitter[words_str[w_idx + i]])\n",
    "                average_on_window = np.mean(np.array(window_words), axis=0) if window_words else np.zeros(vec_num)\n",
    "        \n",
    "                words.append(np.mean(np.array([average_on_stemmed, average_on_window]), axis=0))\n",
    "            else:\n",
    "                words.append(glove_twitter[word])\n",
    "\n",
    "            if vector_added:\n",
    "                weights.append(words[-1])\n",
    "\n",
    "    return [word2idx[word] for word in words_str], tags, torch.from_numpy(np.array(weights)), word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_split_dev_test_file(file_path, word2idx, is_tags=True):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in lines:\n",
    "            try:\n",
    "                if is_tags:\n",
    "                    word, tag = line.rstrip().split(\"\\t\")\n",
    "                else:\n",
    "                    word = line.rstrip()\n",
    "                    if word == '':\n",
    "                        raise\n",
    "                \n",
    "                word = word.lower()\n",
    "                if word in word2idx.keys():\n",
    "                    words.append(word2idx[word])\n",
    "                else:\n",
    "                    temp_word = None\n",
    "                    for j in range(1, min(len(word), 5)):\n",
    "                        if temp_word:\n",
    "                            break\n",
    "                        if word[:-j] in word2idx.keys():\n",
    "                            temp_word = word[:-j]\n",
    "                            break\n",
    "                        for i in range(min(len(word), 5)):\n",
    "                            if word[i:-j] in word2idx.keys():\n",
    "                                temp_word = word[i:-j]\n",
    "                                break\n",
    "                    words.append(word2idx[temp_word] if temp_word else word2idx[\"null\"])\n",
    "\n",
    "                tags.append(0 if tag == \"O\" else 1)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_labels, weights_matrix, train_word2idx = open_split_file_and_calc_weights(\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\")\n",
    "dev_words, dev_labels = open_and_split_dev_test_file(\"/home/student/hw2/NER_task_in_NLP/data/dev.tagged\", train_word2idx)\n",
    "# \"/home/student/hw2/NER_task_in_NLP/data/dev.tagged\", train_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we want to create a lstm which will generate new vector representations for the words\n",
    "class LSTM_Vectorizer(nn.Module):\n",
    "    def __init__(self, num_classes, weights_matrix, input_size, hidden_size, hidden2_size, num_stacked_layers, dropout_prob=0.1):\n",
    "        super(LSTM_Vectorizer, self).__init__()\n",
    "        self.num_classes = num_classes  # We have 2 classes, binary.\n",
    "        self.input_size = input_size  # The number of expected features in the input x.\n",
    "        self.hidden_size = hidden_size  # number of features in hidden state.\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()  # TODO: Enter the new and improved glove matrix.\n",
    "        self.vocab_size = num_embeddings\n",
    "        self.emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb_layer.weight = nn.Parameter(weights_matrix, requires_grad=True)\n",
    "        # self.emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)  # lstm\n",
    "        # self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer1 =  nn.Linear(hidden_size, hidden2_size)  # Layer 1 in the LSTM\n",
    "        self.layer2 = nn.Linear(hidden2_size, num_classes)  # Layer 2 in the LSTM\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, word, labels=None):\n",
    "        word = self.emb_layer(word)\n",
    "        # word = self.dropout(word)\n",
    "        word = word.unsqueeze(1).type(torch.FloatTensor).to(self.device)\n",
    "        h_0 = Variable(torch.zeros(self.num_stacked_layers, word.size(0), self.hidden_size)).to(self.device)  # Short term memory.\n",
    "        c_0 = Variable(torch.zeros(self.num_stacked_layers, word.size(0), self.hidden_size)).to(self.device)  # Long term memory.\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(word, (h_0, c_0))  # Perform lstm with relation to input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size)  # Reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.layer1(out)  # First Dense\n",
    "        out = self.relu(out)  # Activation function - Relu\n",
    "        out = self.layer2(out)  # Second layer\n",
    "        out = self.softmax(out) # Activation function - Softmax\n",
    "        # pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "        if labels is None:\n",
    "            return out, None\n",
    "        loss = self.loss(out, labels)\n",
    "        loss.backward(retain_graph=True)  # Set retain_graph=True to keep the computational graph\n",
    "        # TODO: The retain graph wasn't there.\n",
    "        # TODO: Put it back in the train function and remove from here if the clipping doesn't work.\n",
    "        nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetativeDataset(Dataset):\n",
    "    def __init__(self, words, labels):\n",
    "        # Create a path-to-label dictionary\n",
    "        self.words = words\n",
    "        self.tags = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word_idx = self.words[index]\n",
    "        tag = self.tags[index]\n",
    "        word = torch.tensor([word_idx]).squeeze()\n",
    "        data = {\"word\": word, \"labels\": tag}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CompetativeDataset(words=train_words, labels=train_labels)\n",
    "dev_dataset = CompetativeDataset(words=dev_words, labels=dev_labels)\n",
    "datasets = {\"train\": train_dataset, \"dev\": dev_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_stacked_layers = 1  # Number of stacked lstm layers, in this model, we do not stack layers.\n",
    "num_classes = 2  # Number of output classes\n",
    "\n",
    "lstm_model = LSTM_Vectorizer(num_classes, weights_matrix, vec_num, int(vec_num/2), vec_num*2, num_stacked_layers)  # Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()  # Cross entropy loss\n",
    "optimizer = Adam(lstm_model.parameters(), lr=learning_rate)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24772/3503354684.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.softmax(out) # Activation function - Softmax\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[194], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_sets, optimizer, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()    \n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 38\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     pred \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     40\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((predictions, pred), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[210], line 48\u001b[0m, in \u001b[0;36mLSTM_Vectorizer.forward\u001b[0;34m(self, word, labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(out, labels)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, loss\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "lstm_model = train(model=lstm_model, data_sets=datasets, optimizer=optimizer, num_epochs=15)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_vector_old(word, data):\n",
    "    temp_word = False\n",
    "    if word not in glove_twitter.key_to_index:\n",
    "        # print(f\"{word} not an existing word in the model\")\n",
    "        # if you dont have this word - just skip it\n",
    "        # return False\n",
    "        \n",
    "        if word.startswith(\"http\"):\n",
    "            # all links in train data are tagged O\n",
    "            data.append(np.zeros(vec_dim))\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # check if word is a number\n",
    "            float(word)\n",
    "            data.append(np.zeros(vec_dim))\n",
    "            return\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # try stemming\n",
    "        for i in range(min(len(word), 5)):\n",
    "            if word[i:] in glove_twitter.key_to_index:\n",
    "                temp_word = word[i:]\n",
    "                break\n",
    "            for j in range(1, min(len(word), 5)):\n",
    "                if word[i:-j] in glove_twitter.key_to_index:\n",
    "                    temp_word = word[i:-j]\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        temp_word = word\n",
    "\n",
    "\n",
    "    if temp_word:\n",
    "        vec = glove_twitter[temp_word]\n",
    "        data.append(vec)\n",
    "\n",
    "    else:\n",
    "        data.append(np.zeros(vec_dim))\n",
    "        # print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\") as f:\n",
    "    lines = f.readlines()\n",
    "    words_count = 0\n",
    "    prev_word = ''\n",
    "    for i, line in enumerate(lines):\n",
    "        try:\n",
    "            word, tag = line.rstrip().split(\"\\t\")\n",
    "            words_count += 1\n",
    "            if  (word.startswith('http')):\n",
    "                # print(tag)\n",
    "                if tag != 'O': #and lines[i+1].rstrip().split('\\t')[0] == ':':\n",
    "                    print(\"yes\")\n",
    "                    print(word)\n",
    "            #if line[i-1] != \"\\t\\n\" and \"@\" in word:#  \\\n",
    "            #and line[i-1].rstrip().split(\"\\t\")[0] == \"RT\" and line[i+1].rstrip().split(\"\\t\")[0] == \":\":\n",
    "                # print(line[i-1])\n",
    "                #pass\n",
    "\n",
    "        except:\n",
    "            # print(line.rstrip().split(\"\\t\"))\n",
    "            # # print(line)\n",
    "            # print([line])\n",
    "            # ! the line is blank representing \\t\\n\n",
    "            # TODO thingy for end of sentence\n",
    "            prev_word = ''\n",
    "            continue\n",
    "        prev_word = word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
