{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "\n",
    "vec_num = 50\n",
    "GLOVE_PATH = f'glove-twitter-{vec_num}'\n",
    "glove_twitter = downloader.load(GLOVE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def open_and_split_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        words_str = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in lines:\n",
    "            try:\n",
    "                word, tag = line.rstrip().split(\"\\t\")\n",
    "                word = word.lower()\n",
    "                if (word not in glove_twitter):\n",
    "                    words.append(np.zeros(vec_num))\n",
    "                else:\n",
    "                    words.append(glove_twitter[word])\n",
    "                tags.append(0 if tag == \"O\" else 1)\n",
    "                words_str.append(word)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "    return words, tags, words_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_labels, train_words_str = open_and_split_file(\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words, dev_labels, dev_words_str = open_and_split_file(\"/home/student/hw2/NER_task_in_NLP/data/dev.tagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5860995850622407"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(train_words, train_labels)\n",
    "y_pred = knn.predict(dev_words)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(dev_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        # Create a path-to-label dictionary\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "            words = []\n",
    "            tags = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    word, tag = line.rstrip().split(\"\\t\")\n",
    "                    word = word.lower()\n",
    "                    if (word not in glove_twitter):\n",
    "                        words.append(np.zeros(vec_num))\n",
    "                    else:\n",
    "                        words.append(glove_twitter[word])\n",
    "                    tags.append(0 if tag == \"O\" else 1)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word = self.words[index]\n",
    "        tag = self.tags[index]\n",
    "        word = torch.FloatTensor(word).squeeze()\n",
    "        data = {\"word\": word, \"labels\": tag}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(path=\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\")\n",
    "dev_dataset = CustomDataset(path=\"/home/student/hw2/NER_task_in_NLP/data/dev.tagged\")\n",
    "datasets = {\"train\": train_dataset, \"dev\": dev_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, vec_dim, num_classes, hidden_dim=100):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.first_layer = nn.Linear(vec_dim, hidden_dim)\n",
    "        self.second_layer = nn.Linear(hidden_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, word, labels=None):\n",
    "        x = self.first_layer(word)\n",
    "        x = self.activation(x)\n",
    "        x = self.second_layer(x)\n",
    "        if labels is None:\n",
    "            return x, None\n",
    "        loss = self.loss(x, labels)\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, data_sets, optimizer, num_epochs: int, batch_size=16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_loaders = {\"train\": DataLoader(data_sets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "                    \"dev\": DataLoader(data_sets[\"dev\"], batch_size=batch_size, shuffle=False)}\n",
    "    model.to(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch in data_loaders['train']:\n",
    "            batch_size = 0\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "                batch_size = v.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model(**batch)\n",
    "            loss.backward()  # The important part\n",
    "            optimizer.step()\n",
    "                \n",
    "    # Now use the dev dataset to evaluate the model.\n",
    "    model.eval()\n",
    "    predictions = torch.tensor([])\n",
    "    tags = torch.tensor([])\n",
    "    for batch in data_loaders['dev']:\n",
    "        batch_size = 0\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "            batch_size = v.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()    \n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(**batch) \n",
    "            pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "            predictions = torch.cat((predictions, pred), 0)\n",
    "        tags = torch.cat((tags, (batch[\"labels\"].clone().detach().cpu())), 0)\n",
    "\n",
    "    score = f1_score(tags, predictions)\n",
    "    print(f'F1 score: {score}')\n",
    "                \n",
    "    # with open('model.pkl', 'rb') as f:\n",
    "    #     model = torch.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101276/3324396507.py:29: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  word = torch.FloatTensor(word).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5751898734177215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters that we can change:\n",
    "# hidden_dim: the dimension of the hidden layer\n",
    "# num_epochs: the number of epochs to train the model\n",
    "# learning_rate: the learning rate of the optimizer (Adam) - find out more about it in the documentation.\n",
    "model = FeedForwardNN(vec_num, 2, hidden_dim=int(vec_num*2))\n",
    "optimizer = Adam(params=model.parameters())\n",
    "model = train(model=model, data_sets=datasets, optimizer=optimizer, num_epochs=15)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "X_train_tensors = Variable(torch.Tensor(np.array(train_words)))\n",
    "X_dev_tensors = Variable(torch.Tensor(np.array(dev_words)))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(np.array(train_labels)))\n",
    "y_dev_tensors = Variable(torch.Tensor(np.array(dev_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_dev_tensors = torch.reshape(X_dev_tensors, (X_dev_tensors.shape[0], 1, X_dev_tensors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, hidden2_size, num_stacked_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes  # We have 2 classes, binary.\n",
    "        self.input_size = input_size  # The number of expected features in the input x.\n",
    "        self.hidden_size = hidden_size  # number of features in hidden state.\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        self.seq_length = seq_length  # Number of words in each timestamp.\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)  # lstm\n",
    "        self.layer1 =  nn.Linear(hidden_size, hidden2_size)  # Layer 1 in the LSTM\n",
    "        self.layer2 = nn.Linear(hidden2_size, num_classes)  # Layer 2 in the LSTM\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, word, labels=None):\n",
    "        print(word.size())\n",
    "        word = word.unsqueeze(0)\n",
    "        h_0 = Variable(torch.zeros(self.num_stacked_layers, word.size(0), self.hidden_size)).to(device)  # Short term memory.\n",
    "        c_0 = Variable(torch.zeros(self.num_stacked_layers, word.size(0), self.hidden_size)).to(device)  # Long term memory.\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(word, (h_0, c_0))  # Perform lstm with relation to input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size)  # Reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.layer1(out)  # First Dense\n",
    "        out = self.relu(out)  # Activation function - Relu\n",
    "        out = self.layer2(out)  # Second layer\n",
    "        out = self.softmax(out) # Activation function - Softmax\n",
    "        # pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "        if labels is None:\n",
    "            return out, None\n",
    "        loss = self.loss(out, labels)\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_stacked_layers = 1  # Number of stacked lstm layers, in this model, we do not stack layers.\n",
    "num_classes = 2  # Number of output classes\n",
    "\n",
    "model = LSTM(num_classes, vec_num, int(vec_num/2), vec_num*2, num_stacked_layers, X_train_tensors.shape[1])  # Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()  # Cross entropy loss\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model=model, data_sets=datasets, optimizer=optimizer, num_epochs=15)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model.forward(X_train_tensors)  # Executing forward pass\n",
    "    optimizer.zero_grad()  # Caluclate the gradient, manually setting to 0\n",
    "    \n",
    "    # Obtain the loss function\n",
    "    loss = cross_entropy(outputs, y_train_tensors)\n",
    "    loss.backward()  # Calculates the value of the loss function\n",
    "    \n",
    "    optimizer.step()  # Improve from loss, i.e backpropagation.\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "predictions = torch.tensor([])\n",
    "tags = torch.tensor([])\n",
    "for batch in data_loaders['dev']:\n",
    "    batch_size = 0\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "        batch_size = v.shape[0]\n",
    "\n",
    "    optimizer.zero_grad()    \n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(**batch) \n",
    "        pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "        predictions = torch.cat((predictions, pred), 0)\n",
    "    tags = torch.cat((tags, (batch[\"labels\"].clone().detach().cpu())), 0)\n",
    "\n",
    "score = f1_score(tags, predictions)\n",
    "print(f'F1 score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding matrix according to the glove from earlier.\n",
    "embedding_mat = np.array(train_words)\n",
    "embedding_mat = torch.tensor(embedding_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, embedding_mat, embedding_dim, hidden_dim=50, tag_dim=2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(embedding_mat, freeze=True)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Sequential(nn.ReLU(),\n",
    "                                        nn.Linear(self.hidden_dim, tag_dim))\n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "    \n",
    "\n",
    "    def forward(self, word, sentence_len, tags=None):\n",
    "        embeds = self.word_embedding(word)  # TODO: Check if this is the correct way to use the embedding\n",
    "        lstm_out, _ = self.lstm(embeds.view(1, -1, self.embedding_dim))\n",
    "        tag_space = self.hidden2tag(lstm_out[range(1), sentence_len - 1, :])\n",
    "        tag_score = F.softmax(tag_space, dim=1)\n",
    "        if tags is None:\n",
    "            return tag_score, None\n",
    "        loss = self.loss_fn(tag_score, tags)\n",
    "        return tag_score, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, :len(review)] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataSet(Dataset):\n",
    "    def __init__(self, sentences, sentences_lens, y):\n",
    "        self.X = sentences\n",
    "        self.X_lens = sentences_lens\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.X_lens[item], self.y[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, device, optimizer, train_dataset, val_dataset):\n",
    "    accuracies = []\n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False) #or model.evel()\n",
    "        correct = 0.0\n",
    "        count = 0\n",
    "        accuracy = None\n",
    "        dataset = train_dataset if phase == \"train\" else val_dataset\n",
    "        t_bar = tqdm(dataset)\n",
    "        for sentence, lens, tags in t_bar:\n",
    "            sentence = sentence.type(torch.IntTensor)\n",
    "            lens = lens.type(torch.FloatTensor)\n",
    "            tags = tags.type(torch.FloatTensor)\n",
    "            if phase == \"train\":\n",
    "                tag_scores, loss = model(sentence.to(device), lens.to(device), tags.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    tag_scores, _ = model(sentence.to(device), lens.to(device), tags.to(device))\n",
    "            correct += (tag_scores.argmax(1).to(\"cpu\") == tags).sum()\n",
    "            count += len(tags)\n",
    "            accuracy = correct/count\n",
    "            t_bar.set_description(f\"{phase} accuracy: {accuracy:.2f}\")\n",
    "        accuracies += [accuracy]\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x_train, x_val):\n",
    "    word2idx = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "    idx2word = [\"[PAD]\", \"[UNK]\"]\n",
    "    for sent in x_train:\n",
    "        for word in sent.split():\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx)\n",
    "                idx2word.append(word)\n",
    "\n",
    "    final_list_train, final_list_test = [], []\n",
    "    for sent in x_train:\n",
    "        final_list_train.append([word2idx[word] for word in sent.split()])\n",
    "    for sent in x_val:\n",
    "        final_list_test.append([word2idx[word] if word in word2idx else word2idx['[UNK]'] for word in sent.split()])\n",
    "    return final_list_train, final_list_test, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "(62730, 500) (15733, 500)\n",
      "\n",
      " -- Epoch 0 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/981 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not the same dtype, found input tensor with Double and parameter tensor with Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m -- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     train_accuracy, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_accuracy\u001b[38;5;241m>\u001b[39mbest_accuracy:\n\u001b[1;32m     37\u001b[0m         best_accuracy \u001b[38;5;241m=\u001b[39m val_accuracy\n",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, optimizer, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     17\u001b[0m tags \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     tag_scores, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 15\u001b[0m, in \u001b[0;36mMyNet.forward\u001b[0;34m(self, word, sentence_len, tags)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, sentence_len, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(word)  \u001b[38;5;66;03m# TODO: Check if this is the correct way to use the embedding\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     tag_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden2tag(lstm_out[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m), sentence_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     17\u001b[0m     tag_score \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(tag_space, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not the same dtype, found input tensor with Double and parameter tensor with Float"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "# train_data = pd.Datafram({\"word\" : train_words, \"tag\": train_labels})\n",
    "# test_data = pd.Datafram({\"word\" : dev_words, \"tag\": dev_labels})\n",
    "x_train, y_train = train_words_str, train_labels# train_data[\"word\"].values, train_data[\"tag\"].values\n",
    "x_test, y_test = dev_words_str, dev_labels# test_data[\"word\"].values, test_data[\"tag\"].values\n",
    "n_classes = max(y_test) + 1\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "x_train, x_test, word2idx, idx2word = tokenize(x_train, x_test)\n",
    "vocab_size = len(word2idx)\n",
    "train_sentence_lens = [min(len(s), 500) for s in x_train]\n",
    "test_sentence_lens = [min(len(s), 500) for s in x_test]\n",
    "\n",
    "x_train_pad = padding_(x_train, 500)\n",
    "x_test_pad = padding_(x_test, 500)\n",
    "\n",
    "print(x_train_pad.shape, x_test_pad.shape)\n",
    "\n",
    "train_dataset = ReviewsDataSet(x_train_pad, train_sentence_lens, y_train)\n",
    "test_dataset = ReviewsDataSet(x_test_pad, test_sentence_lens, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "model = MyNet(embedding_mat, vec_num, tag_dim=n_classes)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=8e-3)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_epoch = None\n",
    "for epoch in range(1000):\n",
    "    print(f\"\\n -- Epoch {epoch} --\")\n",
    "    train_accuracy, val_accuracy = train(model, device, optimizer, train_dataloader, test_dataloader)\n",
    "    if val_accuracy>best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "    if epoch - best_epoch == 3:\n",
    "        break\n",
    "print(f\"best accuracy: {best_accuracy:.2f} in epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_vector_old(word, data):\n",
    "    temp_word = False\n",
    "    if word not in glove_twitter.key_to_index:\n",
    "        # print(f\"{word} not an existing word in the model\")\n",
    "        # if you dont have this word - just skip it\n",
    "        # return False\n",
    "        \n",
    "        if word.startswith(\"http\"):\n",
    "            # all links in train data are tagged O\n",
    "            data.append(np.zeros(vec_dim))\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # check if word is a number\n",
    "            float(word)\n",
    "            data.append(np.zeros(vec_dim))\n",
    "            return\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # try stemming\n",
    "        for i in range(min(len(word), 5)):\n",
    "            if word[i:] in glove_twitter.key_to_index:\n",
    "                temp_word = word[i:]\n",
    "                break\n",
    "            for j in range(1, min(len(word), 5)):\n",
    "                if word[i:-j] in glove_twitter.key_to_index:\n",
    "                    temp_word = word[i:-j]\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        temp_word = word\n",
    "\n",
    "\n",
    "    if temp_word:\n",
    "        vec = glove_twitter[temp_word]\n",
    "        data.append(vec)\n",
    "\n",
    "    else:\n",
    "        data.append(np.zeros(vec_dim))\n",
    "        # print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/student/hw2/NER_task_in_NLP/data/train.tagged\") as f:\n",
    "    lines = f.readlines()\n",
    "    words_count = 0\n",
    "    prev_word = ''\n",
    "    for i, line in enumerate(lines):\n",
    "        try:\n",
    "            word, tag = line.rstrip().split(\"\\t\")\n",
    "            words_count += 1\n",
    "            if  (word.startswith('http')):\n",
    "                # print(tag)\n",
    "                if tag != 'O': #and lines[i+1].rstrip().split('\\t')[0] == ':':\n",
    "                    print(\"yes\")\n",
    "                    print(word)\n",
    "            #if line[i-1] != \"\\t\\n\" and \"@\" in word:#  \\\n",
    "            #and line[i-1].rstrip().split(\"\\t\")[0] == \"RT\" and line[i+1].rstrip().split(\"\\t\")[0] == \":\":\n",
    "                # print(line[i-1])\n",
    "                #pass\n",
    "\n",
    "        except:\n",
    "            # print(line.rstrip().split(\"\\t\"))\n",
    "            # # print(line)\n",
    "            # print([line])\n",
    "            # ! the line is blank representing \\t\\n\n",
    "            # TODO thingy for end of sentence\n",
    "            prev_word = ''\n",
    "            continue\n",
    "        prev_word = word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
